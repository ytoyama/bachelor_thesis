\section{関連研究} \label{sec:RelatedResearch}

多カテゴリにおけるレーティング予測に関する従来研究、
及び、提案手法において用いる手法、技術について述べる。
はじめに基礎的な技術について、その後本研究に関連した研究について説明する。


\subsection{形態素解析}

形態素解析とは、文等の文字列を形態素に分割する処理である。
ここで、形態素とは意味を持った言語の最小単位である。
日本語における形態素解析は、単語分割とその語形変化の解析に相当する。
以降、日本語の文字列を形態素解析する場合の単語分割について述べる。
単語分割の手法として、最長一致法と
%分割数最小法、
bi-gramマルコフモデルによるものについて説明する。

最長一致法では、解析すべき文字列を順に読み進めながら
解析結果となる単語を決定していく。
まず、文字列の現在の位置からの部分文字列が辞書内の単語と一致するか検査する。
次に、一致した単語の内最も文字数の多いものを解析された単語として記録する。
最後に、その単語の文字数だけ文字列を読み進め、
再び現在の位置から始まる部分文字列に一致する単語を辞書で検索する。
これを繰り返していくことによって、文字列の終端まで形態素解析を行う。

これに対して、bi-gramマルコフモデルによる手法では、
単語のbi-gram確率を考えこの積が文字列全体で最大となるように文字列を
単語に分割する。
以下に最も適切な単語列$\hat{W}$を求める式を示す。
\begin{gather}
  \hat{W} = \text{argmax}_{W} P(W) \\
  P(W) = \sum^{n}_{i = 1} P(w_i | w_{i - 1}) \nonumber \\
  W = w_1, w_2, ... , w_n \nonumber
\end{gather}
ここで、$W$は元の文字列から分割された単語列であり、
$P(W)$はその単語列の同時確率である。
$w_i$は単語、$P(w_i | w_{i - 1})$は単語$w_i$と単語$w_{i - 1}$の
bi-gram確率である。


\subsection{深層学習}

深層学習とは、多層のニューラルネットワークを用いた機械学習の手法の総称である。
以下には、その内ニューラルネットワークの正則化を行うための2つの手法、
ドロップアウトと重み減衰について述べる。
小節\fullref{ssec:RatingPredictionByNN}で述べる
畳み込みニューラルネットワークも深層学習の手法の一つであるが、
提案手法と関係がないためここでは割愛する。

ドロップアウトとは、ニューラルネットワークにおける層のニューロンの数を
一時的に減らすことによって正則化を行う方法である。
ある層に対してドロップアウトを行うには、その層が持つニューロンの出力値を
確率的に0とする。
また、出力値が0となったニューロンに対する重みの勾配は0となるため、
そのニューロンに対する重みはその学習回において更新されない。
これを各学習回で行うことで、ニューラルネットワーク全体を学習する。

次に、重み減衰について説明する。
重み減衰とは、ニューラルネットワークの各重みについて各重みの大きさに応じて
その重みを学習回毎に減少させる正則化手法である。
重み減衰を行うためには、ニューラルネットワークの最小化すべき目的関数に対して
各重みの2ノルムを足し合わせる。
式\ref{eq:NNObjectiveWithWeightDecay}に、重み減衰を適用した目的関数$L'$示す。
\begin{gather} \label{eq:NNObjectiveWithWeightDecay}
  L' = L + \frac{\lambda}{2} \sum_{\mathbf{w}} {|\mathbf{w}|}^2
\end{gather}
ここで、$L$はニューラルネットワークの重み減衰を適用していない目的関数である。
$\mathbf{w}$はニューラルネットワークの各層における重みである。
$\lambda$は重みの減衰率である。
式\ref{eq:NNObjectiveWithWeightDecay}により、
ニューラルネットワークのある層の重み$\mathbf{w}$に対する
更新式は式\ref{eq:WeightUpdateEquation}のようになる。
\begin{gather} \label{eq:WeightUpdateEquation}
  \mathbf{w} \leftarrow \mathbf{w} - \frac{\partial L}{\partial \mathbf{w}}
                                   - \lambda \mathbf{w}
\end{gather}
ここで、$a \leftarrow b$は変数$a$の値をそのときの式$b$の値で
更新することを示す。
ただし、全結合層のバイアス等では一般に重み減衰を行わない。
これはそのような重みの値は場合によって大きい値を取る必要があるためである。


\subsection{隠れ状態を用いたホテルレビューのレーティング予測}

藤谷ら\cite{fujitani15}は複数のカテゴリにおけるレーティング予測に対して、
Multi-Instance Multi-Label learning for Relation Extraction (MIML-RE)
\cite{mihai12}モデルを用いた手法を提案している。
その手法では、レビュー内の各文毎に予測した隠れレーティングから
レビュー全体のレーティングを予測する。
図\ref{fig:FujitaniRelationsAmongRatingCategories}のように、
文毎のレーティングからレビュー全体のレーティングを予測する際の
カテゴリ間の繋がりを手動で変化させカテゴリ間の関係性を考慮している。
各文の素性にはBag Of Words (BOW)またはBag Of n-gramsを用いている。
各文毎に隠れレーティングを予測することによって
0.4832の正答率が得られることが示された。
また、カテゴリ間の繋がりによって正答率が変化することも示されている。

\begin{figure}
  \includegraphics[0.5]
      {fig/fujitani_miml_relations_among_rating_categories.pdf}
  \caption{藤谷ら\cite{fujitani15}のモデルにおけるカテゴリ同士の繋ぎ方の例}
  \label{fig:FujitaniRelationsAmongRatingCategories}
\end{figure}

この手法では、文同士の位置関係を考慮しておらず、
カテゴリ間については考慮しているものの複雑な関係を捉えることができていない。


\subsection{パラグラフベクトル}

パラグラフベクトルは、文や文書といった大きな単位の言語表現の意味表現を
学習する手法である。
これは、Continuous BOW (CBOW)またはSkip-gram\cite{yoshua03}という
単語の意味表現の学習手法を応用した手法である。
ここではCBOWを応用したDistributed Memory model of Paragraph Vectors (PV-DM)
について説明する。
PV-DMはBOWと異なり、単語の並び順を考慮した文や文書の分散表現を
生成することができる。

\begin{figure}
  \includegraphics[0.5]{fig/paragraph_vector_v2.pdf}
  \caption{パラグラフベクトルの学習の概略}
  \label{fig:ParagraphVector}
\end{figure}

以下に具体的なアルゴリズムを示す。
ここでは文書の意味表現を学習する場合について考える。
学習の概略を図\ref{fig:ParagraphVector}に示す。
まず、意味表現を学習する対象となる文書に含まれる単語を
初めから一つずつ読んでいく。
その際、現在の単語及びその周辺の単語、現在の文書について、
式\ref{eq:ParagraphVector}に示す目的関数$L$を最大化するように
各パラメータの学習を行う。
\begin{gather}
  L = \frac{1}{T} \sum^{T}_{t = k} \log p(w_t | w_{t-k}, ..., w_{t-1}),
    \label{eq:ParagraphVector} \\
  p(w_t | w_{t-k}, ..., w_{t-1}) = \frac{e^{y_{w_t}}}{\sum_i e^{y_i}},
    \nonumber \\
  y = b + Uh(w_{t-k}, ..., w_{t-1}, d; W, D) \nonumber
\end{gather}
ここで、$d$は文書、$w_i$は単語、$W$は全ての単語の分散表現を表す行列、
$D$は全ての文書の分散表現を表す行列である。
$k$はウィンドウサイズ、$T$は現在の文書に含まれる単語数である。
ある単語の周辺を表す区間をウィンドウという。
$p$はsoftmax関数により正規化された、文脈から現在の単語が導かれることの
尤度である。
$p$を構成する$y$は現在の単語とウィンドウ内の単語及び現在の文書から導出される。
$h(w_{t-k}, ..., w_{t-1}, d; W, D)$は引数となるベクトルを平均したベクトル
または結合したベクトルを返す関数である。

PV-DMによって得られたパラグラフベクトルはレーティング予測において
BOW等に比べ高い正答率を示すことが示されている。
しかし、文書全体にパラグラフベクトルを用いる場合、文同士の位置関係が
予測時に考慮できない。


\subsection{ニューラルネットワークを用いたレーティング予測}
\label{ssec:RatingPredictionByNN}

ニューラルネットワークを用いたレーティング予測の手法が、Nalら\cite{nal14}、
Rieら\cite{rie14}、Duyuら\cite{duyu15}等によって提案されている。
これらの方法に共通するのは、単語の意味表現から畳み込みニューラルネットワークと
全結合ニューラルネットワークを用いて分類を行うことである。
畳み込みニューラルネットワークについては後述する。
まず、単語の意味表現から畳み込みニューラルネットワークを
用いて単語同士の関係を捉えた特徴量を抽出する。
その後、そこから得られた文書全体の特徴量を
全結合ニューラルネットワークの入力とし多値または二値分類を行う。
また、Duyuら\cite{duyu15}とNalら\cite{nal14}の手法は
ニューラルネットワークのモデルの中にパラメータとして
単語の意味表現を取り込んでいる。
これにより、特定の分類問題に対してそれらを最適化することができる。

これらの手法は1つのカテゴリにおける多値または二値分類を対象としている。
よって、多カテゴリのレーティング予測において、これらの手法をカテゴリ毎に
適用しただけではカテゴリ間の関係を考慮することができない。

ここで、畳み込みニューラルネットワークについて、説明する。
畳み込みニューラルネットワークとは、畳み込み層とプーリング層を用いた
ニューラルネットワークである。
一般に、畳み込み層とプーリング層は交互に配置される。
畳み込みニューラルネットワークは入力の局所的な特徴を抽出することができる。
また、畳み込みニューラルネットワークは元々画像認識に応用されていた手法であり、
その入力は複数のチャネルを持つことがある。
チャネルとは、画像でいうRGBの各色のことである。
例として、畳み込みニューラルネットワークの入力をRGB画像とした場合、
それは（チャネル）×（画像の幅）×（画像の高さ）の3次元行列で表される。
以下では、複数チャネルを持つ画像に対して畳み込みニューラルネットワークを
適用する場合の畳み込み層とプーリング層について説明する。

畳み込み層とは、前の層の各ニューロンが
それと位置の近い次の層のニューロンとしか結合しないように
全結合層を単純化した層である。
具体的にはある$H×H$の大きさを持つフィルタを考え、
それを畳み込み層の入力値に適用して値を出力する。
図\ref{fig:ConvolutionalLayer}にその概略を示す。
\begin{figure} \label{fig:ConvolutionalLayer}
  \includegraphics[0.7]{fig/convolutional_layer_with_one_filter.pdf}
\end{figure}
式\ref{eq:ConvolutionalLayerFilter}に、畳み込み層のフィルタ$f$によって
処理された出力値$u_{fij}$を示す。
\begin{gather} \label{eq:ConvolutionalLayerFilter}
  u_{fij} = \sum^{C - 1}_{c = 0} \sum^{H - 1}_{p = 0} \sum^{H - 1}_{q = 0}
            z^{(l - 1)}_{c, i + p, j + q} h_{fcpq} + b_{fij}
\end{gather}
ここで、$z^{(l - 1)}_{c, i + p, j + q}$は、前の層$l - 1$の出力値である。
$H$はフィルタの幅である。
$h_{fcpq}$はチャネル$c$に対するフィルタ$f$の重みである。
$b_{fij}$はフィルタ$f$のバイアスである。
ただし、バイアスの値は位置によらず一つのフィルタ内で共有することが多い。
$(i, j)$は出力画像における位置である。
$(p, q)$はフィルタ$f$における位置である。
$u_{fij}$の値に活性化関数を適用し最終的な畳み込み層の出力値$z_{fij}$を
計算する。
\begin{gather} \label{eq:ConvolutionalLayerOutput}
  z_{fij} = \sigma(u_{fij})
\end{gather}
ここで、$\sigma$は活性化関数である。

次に、プーリング層とは入力値の局所的な平均や最大値を取る層である。
これによって、入力値における特徴の位置に関するノイズを緩衝できる。
式\ref{eq:PoolingLayer}に、平均プーリング層の出力値の位置$(i, j)$における
出力値$u_{cij}$を示す。
\begin{gather} \label{eq:PoolingLayer}
  u_{cij} = \frac{1}{H^2} \sum_{(p, q) \in P_{ij}} z_{cpq}
\end{gather}
ここで、$H$はプーリングする範囲の幅である。
$P_{ij}$は出力画像の位置によるプーリングの範囲であり、位置の集合で表される。
$c$はチャネルである。
$(i, j)$は出力画像における位置である。
$(p, q)$はあるプーリングの範囲$P_{ij}$における入力画像の位置である。


\subsection{Adam}

Adam\cite{diederik15}は確率的最適化のためのパラメータ更新のアルゴリズムである。
実験によって、Adamがニューラルネットワークに適用できること、及び、
パラメータをSGDやAdamGradより速く収束させることが確かめられている。
式\ref{eq:AdamUpdate}にAdamによるパラメータ$\mathbf{w}$の更新式を示す。
\begin{gather} \label{eq:AdamUpdate}
  \mathbf{w}_t = \mathbf{w}_{t - 1}
                 - \alpha \frac{\mathbf{m}_t}{\sqrt{\mathbf{v}_t}}, \\
  \mathbf{m}_t = \frac{1 - \beta_1}{1 - {\beta_1}^t}
                 \sum^t_{i = 1} {\beta_1}^{t - i} \mathbf{g}_i, \nonumber \\
  \mathbf{v}_t = \frac{1 - \beta_2}{1 - {\beta_2}^t}
                 \sum^t_{i = 1} {\beta_2}^{t - i} \mathbf{g}_i, \nonumber \\
  \mathbf{g}_i = \frac{\partial L_i}{\partial \mathbf{w}_i} \nonumber
\end{gather}
ここで、$\mathbf{w}_t$は更新回数$t$におけるパラメータ$\mathbf{w}$を表す。
$\mathbf{m}_t$と$\mathbf{v}_t$は更新回数$t$における
パラメータ$\mathbf{w}$の一次・二次モーメントである。
$\alpha$と$\beta_1$、$\beta_2$はAdamのパラメータである。
$\mathbf{g}_t$は更新回数$t$におけるパラメータ$\mathbf{w}$の勾配である。

Adamの特徴として、$\alpha \frac{\mathbf{m}_t}{\sqrt{\mathbf{v}_t}}
\underset{\approx}{<} \alpha$である。
$\alpha \frac{\mathbf{m}_t}{\sqrt{\mathbf{v}_t}}$は全ての$t$について
$g_t$を定数倍しても変化しない。
よって、更新幅のおおまかな上限をデータに依らず$\alpha$によって
決定することができる。
また、Diederikら\cite{diederik15}は
$\frac{\mathbf{m}_t}{\sqrt{\mathbf{v}_t}}$をsignal-to-noise ratio (SNR)と
呼んでいる。
分子$\mathbf{m}_t$は、更新回数$t$までの更新幅が小さい、あるいは、
勾配の向きがよく変わっているとき小さくなる。
よって、パラメータが最適解や局所解、鞍点付近にあるときは更新幅が小さくなり、
それ以外のとき大きくなる。
分母$\mathbf{v}_t$は、更新回数$t$までの更新幅が大きいとき大きくなる。
よって、パラメータが更新回数$t$までに大きく更新されているとき
更新幅は小さくなる。
